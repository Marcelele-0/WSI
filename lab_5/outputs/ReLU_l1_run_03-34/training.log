INFO - Starting training with config: n_samples=1000, normalization=l1
INFO - Logs will be saved to: /home/licho/.programming/UNI/WSI/lab_5/outputs/ReLU_l1_run_03-34/training.log
INFO - Starting neural network training with CUDA propagation...
INFO - Training neural network for 1000 epochs with learning_rate=0.1
INFO - Using ReLU activation in hidden layer
INFO - Running on: CPU
INFO - Epoch 0, Average Loss: 0.065598
INFO - Epoch 100, Average Loss: 0.006827
INFO - Epoch 200, Average Loss: 0.005802
INFO - Epoch 300, Average Loss: 0.004347
INFO - Epoch 400, Average Loss: 0.004073
INFO - Epoch 500, Average Loss: 0.003830
INFO - Epoch 600, Average Loss: 0.003903
INFO - Epoch 700, Average Loss: 0.003352
INFO - Epoch 800, Average Loss: 0.003341
INFO - Epoch 900, Average Loss: 0.003211
INFO - Training completed. Final loss: 0.003058
INFO - Final predictions vs targets:
INFO - Sample 0: Input=[ 0.08599915 -0.91400087], Target=0.0000, Prediction=0.0000
INFO - Sample 1: Input=[ 0.29395902 -0.706041  ], Target=0.0000, Prediction=0.0000
INFO - Sample 2: Input=[ 0.55791366 -0.44208628], Target=0.0000, Prediction=0.0000
INFO - Sample 3: Input=[-0.81680655  0.18319349], Target=0.0000, Prediction=0.0010
INFO - Sample 4: Input=[-0.4442129 -0.5557871], Target=1.0000, Prediction=1.0000
INFO - Sample 5: Input=[ 0.39993384 -0.60006624], Target=0.0000, Prediction=0.0000
INFO - Sample 6: Input=[-0.7856845   0.21431543], Target=0.0000, Prediction=0.0010
INFO - Sample 7: Input=[ 0.37206653 -0.62793344], Target=0.0000, Prediction=0.0000
INFO - Sample 8: Input=[-0.43767524  0.56232476], Target=0.0000, Prediction=0.0010
INFO - Sample 9: Input=[-0.44149143  0.5585086 ], Target=0.0000, Prediction=0.0010
INFO - Neural network training completed successfully!
